{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to introduce students to the [Snorkel](http://www.snorkel.org) tool and the possibilities of programmatic label generation using the weak-supervised learning paradigm.\n",
    "\n",
    "In order to use weakly supervised learning to generate labels, it is necessary to create three datasets:\n",
    "\n",
    "- **train set**: which does not have any labels\n",
    "- **validation set**: used for hyperparameter optimization, has labels\n",
    "- **test set**: used only for final model evaluation, has labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling functions\n",
    "\n",
    "The first step will be to load the dataset and split it into a train set and a test set. Since in our set all SMS have a label, we will simulate a weakly supervised learning problem by randomly removing 80% of the labels. Additionally, Snorkel requires numeric labels, so we need to recode the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:04.995959Z",
     "start_time": "2021-05-19T19:35:04.879076Z"
    }
   },
   "outputs": [],
   "source": [
    "!head data/smsspamcollection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:05.564736Z",
     "start_time": "2021-05-19T19:35:05.335569Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('max_colwidth', 600)\n",
    "\n",
    "SPAM = 1\n",
    "HAM = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "df = pd.read_csv('./data/smsspamcollection.csv', sep='\\t', header=None, names=['old_label', 'text'])\n",
    "\n",
    "df['label'] = df.old_label.apply(lambda x: SPAM if x == 'spam' else HAM)\n",
    "\n",
    "df.loc[df.sample(frac=0.8).index, 'label'] = ABSTAIN\n",
    "df.drop(columns=['old_label'], inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:05.899853Z",
     "start_time": "2021-05-19T19:35:05.891864Z"
    }
   },
   "outputs": [],
   "source": [
    "abstain_idx = df.label == ABSTAIN\n",
    "\n",
    "df_train = df[abstain_idx]\n",
    "df_test = df[~abstain_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple keyword search\n",
    "\n",
    "As a first example, we will use a search for the words \"check\" and \"free\" in SMS content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:07.160530Z",
     "start_time": "2021-05-19T19:35:06.743723Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def check(sms):\n",
    "    return SPAM if \"check\" in sms.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def free(sms):\n",
    "    return SPAM if \"free\" in sms.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to apply the labeling functions to the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:07.664922Z",
     "start_time": "2021-05-19T19:35:07.537784Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [check, free]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of applying the set of labeling functions to the train set is a matrix of size $m \\times n$, where $m$ is the number of examples and $n$ is the number of labeling functions. The matrix contains the result of applying each function to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:08.394943Z",
     "start_time": "2021-05-19T19:35:08.383414Z"
    }
   },
   "outputs": [],
   "source": [
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:08.802557Z",
     "start_time": "2021-05-19T19:35:08.794050Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.iloc[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to analyze this is to determine the coverage of labeling functions (i.e., the percentage of cases for which the function returned a result other than `ABSTAIN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:09.658152Z",
     "start_time": "2021-05-19T19:35:09.652780Z"
    }
   },
   "outputs": [],
   "source": [
    "coverage_check, coverage_free = (L_train != ABSTAIN).mean(axis=0)\n",
    "\n",
    "print(f\"Coverage for check(): {coverage_check * 100:.1f}%\")\n",
    "print(f\"Coverage for free(): {coverage_free * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, Snorkel offers additional tools that allow for deeper analysis of the result of labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:10.422401Z",
     "start_time": "2021-05-19T19:35:10.392383Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of each column is as follows:\n",
    "- `Polarity`: the set of labels returned by the function\n",
    "- `Coverage`: the percentage of examples for which the function returns a value other than `ABSTAIN`\n",
    "- Overlaps: the percentage of examples for which at least one other labeling function returned a value\n",
    "- Conflicts: the percentage of examples for which at least one other labeling function returned a different value\n",
    "\n",
    "If the train set contained labels, the method would also return:\n",
    "- `Correct`: the number of correct labels\n",
    "- `Incorrect`: number of incorrect labels\n",
    "- `Empirical Accuracy`: the percentage of correct labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the examples labeled by the `free()` function as spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:11.544719Z",
     "start_time": "2021-05-19T19:35:11.527030Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.iloc[L_train[:,1] == SPAM].sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the phrase \"call now\" is also a good indicator for spam. So let's add one more labeling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:12.390882Z",
     "start_time": "2021-05-19T19:35:12.232917Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def call_now(sms):\n",
    "    return SPAM if \"call now\" in sms.text.lower() else ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:12.520535Z",
     "start_time": "2021-05-19T19:35:12.493371Z"
    }
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which examples were labeled as spam by the `call_now()` function but omitted by `free()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:13.266968Z",
     "start_time": "2021-05-19T19:35:13.238111Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "buckets = get_label_buckets(L_train[:, 1], L_train[:, 2])\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:13.561565Z",
     "start_time": "2021-05-19T19:35:13.544427Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.iloc[buckets[(ABSTAIN, SPAM)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:14.070016Z",
     "start_time": "2021-05-19T19:35:14.050884Z"
    }
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks as spam all messages containing the word \"HOT\" written in capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:15.054371Z",
     "start_time": "2021-05-19T19:35:15.048930Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def hot(sms):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching based on a regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of labeling function is one that uses regexp to find specific expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:16.266072Z",
     "start_time": "2021-05-19T19:35:15.979235Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@labeling_function()\n",
    "def regex_I_am_free(sms):\n",
    "    if re.search(r\"I\\s.*free\", sms.text, flags=re.I):\n",
    "        return HAM\n",
    "    elif re.search(r\"free\", sms.text, flags=re.I):\n",
    "        return SPAM\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now, regex_I_am_free]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare examples that the `free()` function labels as spam and the `regex_I_am_free()` function considers valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:17.020431Z",
     "start_time": "2021-05-19T19:35:16.996219Z"
    }
   },
   "outputs": [],
   "source": [
    "buckets = get_label_buckets(L_train[:, 1], L_train[:, 3])\n",
    "df_train.iloc[buckets[(SPAM, HAM)]].sample(10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that will mark as spam all messages containing any amounts specified with a currency symbol ($99, £1.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:18.671866Z",
     "start_time": "2021-05-19T19:35:18.669094Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def contains_money(sms):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching based on heuristics\n",
    "\n",
    "A simple heuristic to find spam is to assume that if more than 10% of the message text is written in capitals, there is a good chance it is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:20.563651Z",
     "start_time": "2021-05-19T19:35:20.207843Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def has_many_uppercase_words(sms):\n",
    "    percentage_uppercase = sum([word.isupper() for word in sms.text.split()]) / len(sms.text.split())\n",
    "    \n",
    "    return SPAM if percentage_uppercase > 0.1 else ABSTAIN\n",
    "\n",
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks as valid those messages that are shorter than 10 words and do not contain any word written in capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:21.341099Z",
     "start_time": "2021-05-19T19:35:21.333785Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def short_and_no_uppercase(sms):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an external statistical model\n",
    "\n",
    "When labeling data, you can use external models whose response can be important information for deciding how to label an example. Snorkel has several built-in integrations in the form of the `Preprocessor` interface, in the example below we will use the `SpaCy` library to perform additional grammatical analysis of the text. However, you will need to download the English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:31:22.242830Z",
     "start_time": "2021-05-19T19:31:03.524220Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:31:34.018309Z",
     "start_time": "2021-05-19T19:31:33.068241Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:26.090920Z",
     "start_time": "2021-05-19T19:35:25.099419Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:26.870094Z",
     "start_time": "2021-05-19T19:35:26.810785Z"
    }
   },
   "outputs": [],
   "source": [
    "_text = \"\"\"I don't England is a country that is part of the United Kingdom. \n",
    "It shares land borders with Wales to its west and Scotland to its north. \n",
    "The Irish Sea lies northwest of England and the Celtic Sea to the southwest. \n",
    "England is separated from continental Europe by the North Sea to the east and the \n",
    "English Channel to the south. The country covers five-eighths of the island of \n",
    "Great Britain, which lies in the North Atlantic, and includes over 100 smaller islands, \n",
    "such as the Isles of Scilly and the Isle of Wight.\"\"\"\n",
    "\n",
    "doc = nlp(_text)\n",
    "\n",
    "for e in doc.ents:\n",
    "    print(e.text, e.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:28.056838Z",
     "start_time": "2021-05-19T19:35:27.497633Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that short text messages in which a reference to a specific person appears are not spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:28.564396Z",
     "start_time": "2021-05-19T19:35:28.552608Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:35:29.352633Z",
     "start_time": "2021-05-19T19:35:29.348630Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function(pre=[spacy])\n",
    "def has_person(sms):\n",
    "    if len(sms.doc) < 20 and any([ent.label_ == \"PERSON\" for ent in sms.doc.ents]):\n",
    "        return HAM\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:09.731132Z",
     "start_time": "2021-05-19T19:35:29.949300Z"
    }
   },
   "outputs": [],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words, has_person]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of pre-processing data for labeling would be determining the average word frequency of a document. Below we define a function that determines the average word frequency and we decorate it as an example of a pre-processor. When a text message is sent to the next labeling function, the pre-processor will populate the text message with the average word frequency and, based on that, the labeling function will make a decision (we assume that if the text message contains many rare words then it is spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:32.970893Z",
     "start_time": "2021-05-19T19:37:32.784823Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def avg_word_freq(sms):\n",
    "    sms.avg_word_freq = sum([zipf_frequency(word, 'en') for word in sms.text.split()]) / len(sms.text.split())\n",
    "    \n",
    "    return sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:34.795326Z",
     "start_time": "2021-05-19T19:37:34.790325Z"
    }
   },
   "outputs": [],
   "source": [
    "@labeling_function(pre=[avg_word_freq])\n",
    "def many_rare_words(sms):\n",
    "    return ABSTAIN if sms.avg_word_freq >= 4 else SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:39.197040Z",
     "start_time": "2021-05-19T19:37:35.984206Z"
    }
   },
   "outputs": [],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_many_uppercase_words, has_person, many_rare_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:39.849049Z",
     "start_time": "2021-05-19T19:37:39.827079Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.iloc[L_train[:,6] == SPAM].sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Write a labeling function that marks messages containing more than 3 adjectives as spam. Use the SpaCy library for pre-processing. \n",
    "\n",
    "__Hint__: the following example shows how to read the part-of-speech label for each token from the message being analyzed. For information on all token properties recognized by SpaCy, see [API documentation](https://spacy.io/api/token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:37:41.894274Z",
     "start_time": "2021-05-19T19:37:41.111265Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sms = \"Yetunde, i'm sorry but moji and i seem too busy to be able to go shopping.\"\n",
    "\n",
    "for token in nlp(sms):\n",
    "    print(f\"{token.text:<10} {token.pos_:<10} {token.tag_:<10} {token.lemma_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining labeling functions into a single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of labeling functions is not to achieve individually large coverage. Labeling functions are inherently noisy and can make many individual errors. The true utility of labeling functions becomes apparent when multiple functions are combined to form a single model.\n",
    "\n",
    "We will first build a simple model based on majority voting, and then build a more complex model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:10.800140Z",
     "start_time": "2021-05-19T19:37:42.829275Z"
    }
   },
   "outputs": [],
   "source": [
    "lfs = [check, free, call_now, regex_I_am_free, has_person, many_rare_words]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:11.685278Z",
     "start_time": "2021-05-19T19:38:11.659189Z"
    }
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_test, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:13.165490Z",
     "start_time": "2021-05-19T19:38:12.488067Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:14.046160Z",
     "start_time": "2021-05-19T19:38:14.041527Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:14.971642Z",
     "start_time": "2021-05-19T19:38:14.958011Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels, counts = np.unique(preds_train, return_counts=True)\n",
    "\n",
    "for l, c in zip(labels, counts):\n",
    "    print(f\"LABEL: {l}, count: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:16.095821Z",
     "start_time": "2021-05-19T19:38:15.735200Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:16.707091Z",
     "start_time": "2021-05-19T19:38:16.660020Z"
    }
   },
   "outputs": [],
   "source": [
    "majority_acc = majority_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Majority voting accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=df_test.label, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Probabilistic model accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, some data points will not receive any label. It is necessary to filter out these points before sending the labeling result for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:18.328146Z",
     "start_time": "2021-05-19T19:38:18.257149Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.utils import preds_to_probs, probs_to_preds\n",
    "\n",
    "preds_train, probs_train = label_model.predict(L=L_train, return_probs=True)\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(X=df_train, y=probs_train, L=L_train)\n",
    "df_train.shape, df_train_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we were able to quickly prepare labels for about 650 examples (recall that initially no example in the `df_train` set had labels).\n",
    "\n",
    "The next step will use prepared labels as training data for the actual classifier. We will use simple [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), first pre-processing the input data. Since we are working with text, we will use the [word vector representation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) created based on 5-grams by `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:20.290628Z",
     "start_time": "2021-05-19T19:38:20.140958Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
    "\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:21.403376Z",
     "start_time": "2021-05-19T19:38:21.027861Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sklearn_model = LogisticRegression(C=1e3, solver='lbfgs')\n",
    "sklearn_model.fit(X=X_train, y=preds_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:22.426987Z",
     "start_time": "2021-05-19T19:38:22.421221Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Logistic regression accuracy: {sklearn_model.score(X=X_test, y=df_test.label) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the final model improved the score over the majority vote and the `LabelModel` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Complete the above calls with functions that you wrote yourself and check whether your functions improve the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming functions\n",
    "\n",
    "The idea of a transforming function is to perform an atomic transformation of an instance. For data that is an image, typical transformations include cropping, rotating, and changing the color palette. For text data, you can replace words with synonyms, substitute named entities, cut random pieces of text, etc. In the following example we will find types of named entities occurring in the text, and then prepare a simple transformer that will randomly replace occurrences of the `PERSON` entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:38:31.560232Z",
     "start_time": "2021-05-19T19:38:26.866544Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for doc in nlp.pipe(df_train.text.sample(frac=0.05)):\n",
    "    print(f\"Entities: {[(e.text, e.label_) for e in doc.ents]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:40:36.945027Z",
     "start_time": "2021-05-19T19:39:12.256235Z"
    }
   },
   "outputs": [],
   "source": [
    "person_entities = []\n",
    "\n",
    "for doc in nlp.pipe(df_train.text):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ == 'PERSON':\n",
    "            person_entities.append(e.text)\n",
    "        \n",
    "person_entities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:03.662025Z",
     "start_time": "2021-05-19T19:42:02.919683Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.augmentation import transformation_function\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)\n",
    "\n",
    "@transformation_function(pre=[spacy])\n",
    "def random_person_ner(sms):\n",
    "    person_ners = [e.text for e in sms.doc.ents]\n",
    "    \n",
    "    if person_ners:\n",
    "        person_to_replace = np.random.choice(person_ners)\n",
    "        person_to_add = np.random.choice(person_entities)\n",
    "        sms.text = sms.text.replace(person_to_replace, person_to_add)\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of transformation could be using WordNet to find synonyms for words. However, this requires downloading a corpus of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:20.633409Z",
     "start_time": "2021-05-19T19:42:20.093443Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:25.058494Z",
     "start_time": "2021-05-19T19:42:25.054333Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_synonym(word):\n",
    "    \n",
    "    synsets = wordnet.synsets(word)\n",
    "    \n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        \n",
    "        return np.random.choice([w.replace(\"_\", \" \") for w in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:27.153320Z",
     "start_time": "2021-05-19T19:42:27.148396Z"
    }
   },
   "outputs": [],
   "source": [
    "@transformation_function()\n",
    "def replace_words_with_synonym(sms, num_replacements=5):\n",
    "\n",
    "    words = sms.text.split()\n",
    "    \n",
    "    for _ in range(num_replacements):\n",
    "        word_idx = np.random.choice(range(len(words)))\n",
    "        synonym = get_synonym(words[word_idx])\n",
    "        if synonym:\n",
    "            words[word_idx] = synonym\n",
    "        \n",
    "    sms.text = ' '.join(words)\n",
    "    return sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compare the original text message content with the transformed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:42:31.366022Z",
     "start_time": "2021-05-19T19:42:31.361290Z"
    }
   },
   "outputs": [],
   "source": [
    "# source: https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/utils.py\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def preview_tfs(df, tfs):\n",
    "    transformed_examples = []\n",
    "    for f in tfs:\n",
    "        for i, row in df.iterrows():\n",
    "            transformed_or_none = f(row)\n",
    "            # If TF returned a transformed example, record it in dict and move to next TF.\n",
    "            if transformed_or_none is not None:\n",
    "                transformed_examples.append(\n",
    "                    OrderedDict(\n",
    "                        {\n",
    "                            \"TF Name\": f.name,\n",
    "                            \"Original Text\": row.text,\n",
    "                            \"Transformed Text\": transformed_or_none.text,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "    return pd.DataFrame(transformed_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:46:55.348701Z",
     "start_time": "2021-05-19T19:46:54.714091Z"
    }
   },
   "outputs": [],
   "source": [
    "tfs = [random_person_ner, replace_words_with_synonym]\n",
    "\n",
    "preview_tfs(df_train.sample(frac=0.1), tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying transforming functions requires some policy defining the order and number of transformations. In the example below, two transformation functions are drawn at random and this sequence of two functions is applied twice to each data point. As a result, we triple the size of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:48:27.795346Z",
     "start_time": "2021-05-19T19:48:18.367019Z"
    }
   },
   "outputs": [],
   "source": [
    "from snorkel.augmentation import RandomPolicy, PandasTFApplier\n",
    "\n",
    "random_policy = RandomPolicy(len(tfs), sequence_length=2, n_per_original=2, keep_original=True)\n",
    "\n",
    "tf_applier = PandasTFApplier(tfs, random_policy)\n",
    "\n",
    "df_train_sample = df_train.sample(frac=0.1)\n",
    "df_train_augmented = tf_applier.apply(df_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T19:48:30.188103Z",
     "start_time": "2021-05-19T19:48:30.181349Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_sample.shape, df_train_augmented.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assignment\n",
    "\n",
    "Modify the transforming function ``replace_words_with_synonym()`` so that you can restrict the replacement of words with synonyms only for specific parts of speech (e.g., replace only nouns or verbs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
